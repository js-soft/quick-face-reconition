{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot reload changed source files. See\n",
    "# https://stackoverflow.com/questions/56059651\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython import display\n",
    "import albumentations as alb\n",
    "from src import training, util, augmentation\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Global definitions\n",
    "input_data_dir = \"./data/images\"\n",
    "augmented_images_dir = \"./data/augmented2\"  # output directory for derivative images during augmentation\n",
    "test_data_dir = f\"{augmented_images_dir}/test\"\n",
    "train_data_dir = f\"{augmented_images_dir}/train\"\n",
    "model_dir = \"./data/models\"  # output to save model snapshots during training\n",
    "cuda_device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for dpath in [input_data_dir, augmented_images_dir, test_data_dir, train_data_dir, model_dir]:\n",
    "    os.makedirs(dpath, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recording your face\n",
    "\n",
    "Generate webcam snapshots at 10 fps for around 30 seconds with your face in view and for another 10 seconds without. While recording your face rotate and move around your head.\n",
    "\n",
    "```sh\n",
    "$ mkdir -p ./data/images\n",
    "$ ffmpeg -f v4l2 -framerate 10 -video_size 800x600 -i /dev/video0 -r 10/1 \"./data/images/%03d.png\"\n",
    "```\n",
    "\n",
    "# 2. Manually create bounding boxes\n",
    "\n",
    "Using the program _labelme_, create the bounding boxes for your images. For images without a face set no bounding box.\n",
    "\n",
    "```sh\n",
    "labelme --keep-prev --autosave --labels face --nodata --output ./data/images ./data/images\n",
    "```\n",
    "\n",
    "The resulting directory structure contains the images in png format and eponymous json files which contain the bounding boxes.\n",
    "\n",
    "```sh\n",
    "$ tree ./data/images\n",
    "# ├── 001.json   // images containing our face\n",
    "# ├── 001.png\n",
    "# ├── 002.json\n",
    "# ├── 002.png\n",
    "# ├── ...\n",
    "# ├── 350.json\n",
    "# ├── 350.png\n",
    "# ├── 351.json   // images showing the naked background\n",
    "# ├── 351.png\n",
    "# ├── ...\n",
    "# ├── 425.json\n",
    "# └── 425.png\n",
    "```\n",
    "\n",
    "# 3. Augmentation\n",
    "\n",
    "We extend our dataset by creating derivative images using the [albumentations](https://albumentations.ai/) library, which offers image transformations with a simple interface which we shall use to increase the variance and the volume of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    # We misappropriate the shift-scale-rotate transformation to implement\n",
    "    # a zoom-in and zoom-out transform. Border extrapolation for zoomed-out\n",
    "    # images must use a constant value or else we might produce spurious\n",
    "    # reflections of faces at the borders.\n",
    "    alb.ShiftScaleRotate(shift_limit=0.0, scale_limit=(-0.5, 0.5), rotate_limit=0.0, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0.0),\n",
    "    alb.RandomCrop(width=450, height=450, p=1.0),\n",
    "    alb.HorizontalFlip(p=0.5),\n",
    "    # Manipulate the brightness and gamma to emulate different lighting\n",
    "    # conditions.\n",
    "    alb.RandomBrightnessContrast(p=0.5),\n",
    "    alb.RandomGamma(p=0.75),\n",
    "    # Preprocessing for training. We reduce the size and use grayscale.\n",
    "    alb.Resize(244, 244, p=1.0),\n",
    "    alb.ToGray(p=1.0),\n",
    "]\n",
    "\n",
    "# Generate the samples, separated into training and test data.\n",
    "augmentation.generate_samples(pipeline, 25, input_data_dir, train_data_dir)\n",
    "augmentation.generate_samples(pipeline, 5, input_data_dir, test_data_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We build our model around a pre-trained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = training.VGG16DualHead(freeze_vgg16_weights=True, device=cuda_device)\n",
    "\n",
    "# Set up the data loading pipeline to produce torch tensors and convert images\n",
    "# from numpy to torch image format.\n",
    "transforms = tv.transforms.Compose(\n",
    "    [\n",
    "        tv.transforms.ToTensor(),  # converts image from numpy to torch format\n",
    "        tv.transforms.Lambda(lambda x: t.FloatTensor(x)),\n",
    "    ]\n",
    ")\n",
    "label_transforms = tv.transforms.Compose(\n",
    "    [\n",
    "        tv.transforms.Lambda(lambda x: t.FloatTensor(x)),\n",
    "    ]\n",
    ")\n",
    "train_data = training.DataFromDisk(train_data_dir, transform_x=transforms, transform_y=label_transforms)\n",
    "train_loader = t.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_data = training.DataFromDisk(test_data_dir, transform_x=transforms, transform_y=label_transforms)\n",
    "test_loader = t.utils.data.DataLoader(test_data, batch_size=500)\n",
    "\n",
    "# Configure the optimization algorithm and an adaptive learning rate.\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = t.optim.lr_scheduler.MultiplicativeLR(optimizer, lambda epoch: 0.90)\n",
    "\n",
    "# Run training loop.\n",
    "test_loss_mean = []\n",
    "train_loss_mean = []\n",
    "for epoch_ii in range(50):\n",
    "    training.train_epoch(model, cuda_device, train_loader, optimizer)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate performance on the training and the test set to get a measure\n",
    "    # for how well we're doing. Note that we use the full data set to compute\n",
    "    # the metrics which takes a lot of time. In practice, we could also\n",
    "    # evaluate performance on only a subset of the data, or only every n-th\n",
    "    # epoch.\n",
    "    test_perf = training.eval_performance(model, test_loader)\n",
    "    train_perf = training.eval_performance(model, train_loader)\n",
    "    test_loss_mean.append(test_perf[\"loss_mean\"])\n",
    "    train_loss_mean.append(train_perf[\"loss_mean\"])\n",
    "\n",
    "    # Save model's state_dict.\n",
    "    t.save(model.state_dict(), f\"{model_dir}/face_detector_epoch{epoch_ii:03d}.pth\")\n",
    "\n",
    "    # Display training progress in a graph.\n",
    "    plot = util.plot_training_progress(train_loss_mean, test_loss_mean)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plot)\n",
    "\n",
    "    epoch_ii += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Finally, the trained model can be used to do inference. With some glue code, we're able to draw a predicted bounding box on top of our webcam image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from memory. Choose according to training process\n",
    "model = training.VGG16DualHead.from_state_dict_pth(f\"{model_dir}/face_detector_epoch030.pth\")\n",
    "\n",
    "# Define preprocessor of webcam images\n",
    "def prep_for_model(cam_img):\n",
    "    img = alb.center_crop(cam_img, 600, 600)\n",
    "    img = alb.resize(img, 244, 244)\n",
    "    img = alb.to_gray(img)\n",
    "    img = tv.transforms.functional.to_tensor(img)\n",
    "    return img\n",
    "\n",
    "try:\n",
    "    for cam_img in util.read_cam():\n",
    "        prep_img = prep_for_model(cam_img)\n",
    "        hyp = model(prep_img).detach().cpu().numpy()\n",
    "        bbox, conviction = hyp[:4], hyp[4]\n",
    "\n",
    "        # If our face is in view, draw a bounding box\n",
    "        if conviction > 0.9:\n",
    "            img_w_bbox = util.overlay_bboxes(prep_img, [bbox])\n",
    "        else:\n",
    "            img_w_bbox = prep_img\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(util.ImgType.convert(img_w_bbox, util.ImgType.pil))\n",
    "except KeyboardInterrupt:\n",
    "    pass  # graceful stopping of loop running in notebook\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "Being an instructive guide, the approach described here has several limitations which we shall make explicit.\n",
    "\n",
    "- [ ] VGG16 Limitierungen\n",
    "- [ ] Single Channel Model für Greyscale data (s. VGG Limitiereungen)\n",
    "- [ ] Preprocessing einmal durchführen und als Wurst im Speicher ablegen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c2aed74f7949df51d85c687b039764adff0ccf2326c7e3dd9ab304d0fff0ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
